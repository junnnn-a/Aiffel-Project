{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "446051e2",
   "metadata": {},
   "source": [
    "# EXPLORATION_04. 작사가 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b41ab",
   "metadata": {},
   "source": [
    "## (1). 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d02da",
   "metadata": {},
   "source": [
    "mkdir -p ~/aiffel/lyricist/models  \n",
    "ln -s ~/data ~/aiffel/lyricist/data  \n",
    "위에 코드를 Cloud shell 에 입력 후 데이터를 다운 받아 준비!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb53268",
   "metadata": {},
   "source": [
    "## (2). 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8932c927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "## raw_corpus 라는 빈 리스트를 만들었다. \n",
    "## 그 후에 txt 파일을 모두 읽어서 우리가 만들어 둔 리스트에 담는다!!!!\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6dd6aa",
   "metadata": {},
   "source": [
    "우리가 원하는 데이터만 받아내고 필요없는 문장들은 빼고 보고싶다!  \n",
    "1. 그래서 일단 아무말이 없는 문장을 빼고 싶기 때문에 len(sentence)가 0이면 건너뛰게 할 것이다. (sentence)의 길이!  \n",
    "2. 그리고 다음으로는 문장의 끝이 \":\" 이렇게 끝나면 화자 이름 : 이런느낌이니까 그런 문장도 건너뛰게 해준다.  \n",
    "일단 우리가 필요없는 문장이 잘 빠졌나 10개만 추출해서 확인해보자!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb890a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   \n",
    "    if sentence[-1] == \":\": continue  \n",
    "\n",
    "    if idx > 9: break   \n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e96c42",
   "metadata": {},
   "source": [
    "## (3). 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b713e4c",
   "metadata": {},
   "source": [
    "문장을 일정한 기준으로 쪼갤 것이다. 이 과정을 토큰화라고 한다!!!  \n",
    "사실 띄어쓰기를 기준으로 두는게 제일 간단한데 그 때 문제가 발생할 수 도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea441f",
   "metadata": {},
   "source": [
    "입력된 문장을  \n",
    "(#1) => 소문자로 바꾼다음, 양쪽 공백을 지워준다.  \n",
    "(#2) => 특수문자 양쪽에 공백을 하나 넣는다. 예를 들어 ? 나 ! 같은 거임  \n",
    "(#3) => 여러개의 공백은 하나의 공백으로 바꿔준다.  \n",
    "(#4) => a-zA-Z, ?, !, . , , 뭐 이런 애들이 아니면 하나의 공백으로 바꿔준다.  \n",
    "(#5) => 다시 양쪽 공백을 지워준다.  \n",
    "(#6) => 문장 시작에는 <start>, 끝에는 <end> 를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1085a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "## 이 문장이 어떻게 필터링되는지 확인\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5379e634",
   "metadata": {},
   "source": [
    "너무 긴 문장은 노래 가사를 작사하는데 어울리지 않는다.  \n",
    "=> 그래서 토큰 개수가 15개 넘어가지 않게 split() 으로 제한을 두었다.(밑에 코드!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1128f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 여기에 빈 리스트를 만들어서 아까 위에서 정재 시킨 문장을 넣어준다.\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뛴다.\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    if len(preprocess_sentence(sentence).split()) > 15: continue\n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "## 잘 정재되었는지 확인해보자! 너무 많으니까 10개만!\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44657b51",
   "metadata": {},
   "source": [
    "## (4). 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69135eab",
   "metadata": {},
   "source": [
    "tokenize() 함수로 데이터를 Tensor 로 변환한 후에 , sklearn 모듈의 train_test_split() 함수를 사용해서 훈련데이터랑 평가 데이터를 분리해준다.  \n",
    "(조건!!!!) => 단어장의 크기는 12,000이상이고 총 데이터의 20% 를 평가 데이터셋으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50bfa528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2967 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  118 ...    0    0    0]\n",
      " [   2  258  194 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fe61c262fa0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    ## 12000단어를 기억할 수 있는 tokenizer를 만든다.\n",
    "    ## 12000단어에 포함되지 못한 단어는 '<unk>'로 바꾼다.\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    ## corpus를 이용해 tokenizer 내부의 단어장을 완성한다.\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    ## 준비한 tokenizer를 이용해 corpus를 Tensor로 변환한다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba7f8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4   95  303   62   53    9  946 6263]\n",
      " [   2   15 2967  871    5    8   11 5739    6  374]\n",
      " [   2   33    7   40   16  164  288   28  333    5]]\n"
     ]
    }
   ],
   "source": [
    "## 생성된 텐서 데이터를 3번째 행, 10번째 열까지만 출력해서 데이터 값 확인!\n",
    "\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd5ed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "## 텐서데이터는 전부 정수이다. 숫자는 아까 우리가 tokenizer에 구축된 단어 사전의 인덱스이다.\n",
    "## 단어 사전이 어떻게 구축되었는지 확인해보기!!\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d6a505a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    4   95  303   62   53    9  946 6263    3    0    0    0]\n",
      "[  50    4   95  303   62   53    9  946 6263    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "## tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성한다.\n",
    "\n",
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a030fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train:  (124810, 14)\n",
      "Target Train:  (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=34)\n",
    "\n",
    "print('Source Train: ', enc_train.shape)\n",
    "print('Target Train: ', dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0bacec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "## tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "## 준비한 데이터 소스로부터 데이터셋을 만든다.\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e78c7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 만드는 모델에는 1개의 embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성\n",
    "## 일단 지금은 구조도에 설명된 정도만 이해하고 pass~~~~\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "## embedding 레이어는 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4229c",
   "metadata": {},
   "source": [
    "## (5). 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19d885f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 6.9579597e-05,  7.9778139e-05, -8.0385063e-05, ...,\n",
       "          1.5380612e-05,  1.4584203e-04, -7.1330542e-05],\n",
       "        [-8.6160653e-05,  2.3694332e-05,  2.6869719e-05, ...,\n",
       "         -7.4822819e-05, -8.8675142e-06, -1.5179208e-04],\n",
       "        [-1.3851011e-04,  1.5108044e-04,  3.3125456e-04, ...,\n",
       "         -2.9256468e-04,  7.3004419e-05,  3.7309415e-05],\n",
       "        ...,\n",
       "        [ 6.3403777e-04,  6.3120147e-05,  1.8700586e-03, ...,\n",
       "          1.4739120e-03,  6.2014395e-04,  8.0716040e-04],\n",
       "        [ 6.7187264e-04,  7.4461648e-05,  1.7239354e-03, ...,\n",
       "          1.3620409e-03,  4.4154195e-04,  1.0111111e-03],\n",
       "        [ 7.3472498e-04,  1.2781932e-05,  1.4848493e-03, ...,\n",
       "          1.1072840e-03,  1.5929212e-04,  1.1940884e-03]],\n",
       "\n",
       "       [[ 6.9579597e-05,  7.9778139e-05, -8.0385063e-05, ...,\n",
       "          1.5380612e-05,  1.4584203e-04, -7.1330542e-05],\n",
       "        [ 3.7509244e-04,  9.1152033e-05,  2.2766324e-04, ...,\n",
       "          1.7290680e-04,  8.0630125e-05,  6.1593528e-05],\n",
       "        [ 6.4702862e-04,  3.2424316e-04,  4.4122091e-04, ...,\n",
       "          2.3243969e-04, -8.6509266e-05,  2.8527674e-04],\n",
       "        ...,\n",
       "        [ 7.8991888e-04, -2.5086767e-05,  5.8654579e-04, ...,\n",
       "          4.2958389e-04, -4.5637382e-04,  1.6298027e-03],\n",
       "        [ 1.0131174e-03, -1.5497036e-04,  4.4140901e-04, ...,\n",
       "          1.0247539e-04, -6.6581159e-04,  1.7946813e-03],\n",
       "        [ 1.2385342e-03, -2.8246822e-04,  3.2188147e-04, ...,\n",
       "         -2.2484567e-04, -8.3602912e-04,  1.9786239e-03]],\n",
       "\n",
       "       [[ 6.9579597e-05,  7.9778139e-05, -8.0385063e-05, ...,\n",
       "          1.5380612e-05,  1.4584203e-04, -7.1330542e-05],\n",
       "        [ 1.0023948e-04, -3.6967711e-05, -5.3310581e-05, ...,\n",
       "         -4.4057888e-05,  2.3777802e-04,  1.8538494e-04],\n",
       "        [-1.5077577e-04,  2.4180541e-04, -1.1213890e-04, ...,\n",
       "         -1.4912107e-04,  2.1171293e-04,  3.8605716e-04],\n",
       "        ...,\n",
       "        [ 1.1763361e-03, -7.0847693e-04, -5.5496139e-04, ...,\n",
       "         -1.0679815e-03, -1.0587013e-03,  1.5114681e-03],\n",
       "        [ 1.4113211e-03, -8.2120032e-04, -5.0085317e-04, ...,\n",
       "         -1.2359887e-03, -1.2129662e-03,  1.7781075e-03],\n",
       "        [ 1.6241339e-03, -9.2056277e-04, -4.4347940e-04, ...,\n",
       "         -1.3966543e-03, -1.3306112e-03,  2.0452761e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 6.9579597e-05,  7.9778139e-05, -8.0385063e-05, ...,\n",
       "          1.5380612e-05,  1.4584203e-04, -7.1330542e-05],\n",
       "        [ 2.2903235e-04,  2.7242507e-04, -1.9856017e-04, ...,\n",
       "         -2.5521804e-04,  4.9788912e-05,  1.6702918e-04],\n",
       "        [ 6.4293679e-04,  3.6152574e-04, -3.1740961e-04, ...,\n",
       "         -3.9193738e-04,  7.9944308e-05,  4.3929103e-04],\n",
       "        ...,\n",
       "        [ 7.2759896e-04, -6.4758223e-04,  2.0018590e-04, ...,\n",
       "         -3.2146889e-04, -1.5954877e-03,  1.1467369e-03],\n",
       "        [ 8.8491285e-04, -7.9063454e-04,  1.5493993e-04, ...,\n",
       "         -4.0892119e-04, -1.7544127e-03,  1.2429253e-03],\n",
       "        [ 1.0679750e-03, -9.1076066e-04,  1.1004079e-04, ...,\n",
       "         -5.4412481e-04, -1.8655566e-03,  1.3811076e-03]],\n",
       "\n",
       "       [[ 6.9579597e-05,  7.9778139e-05, -8.0385063e-05, ...,\n",
       "          1.5380612e-05,  1.4584203e-04, -7.1330542e-05],\n",
       "        [-9.5074429e-05,  3.0219674e-04, -1.4787904e-04, ...,\n",
       "          1.0908633e-04,  1.6624112e-04, -2.5555142e-04],\n",
       "        [-2.4594591e-04,  4.2285994e-04,  9.4817711e-05, ...,\n",
       "         -6.4765176e-05,  6.4997068e-05, -1.9220368e-05],\n",
       "        ...,\n",
       "        [-2.3703456e-04,  1.1403975e-05,  9.1299189e-05, ...,\n",
       "         -3.2848975e-04, -6.6676206e-04, -8.8200270e-04],\n",
       "        [ 3.0948847e-04, -2.0956238e-04,  1.3754002e-04, ...,\n",
       "         -3.9064020e-04, -7.1036705e-04, -6.4213894e-04],\n",
       "        [ 5.8180746e-04, -6.8850415e-05,  4.8341177e-04, ...,\n",
       "         -4.1265975e-04, -4.6651930e-04, -3.7703160e-04]],\n",
       "\n",
       "       [[ 6.9579597e-05,  7.9778139e-05, -8.0385063e-05, ...,\n",
       "          1.5380612e-05,  1.4584203e-04, -7.1330542e-05],\n",
       "        [-1.8730876e-04, -9.5420262e-05, -1.4378344e-04, ...,\n",
       "          4.6567115e-05,  3.0675138e-04, -5.5322553e-06],\n",
       "        [-2.5622346e-04, -8.6090819e-05, -1.8743836e-04, ...,\n",
       "         -3.0131583e-05,  4.8458617e-04, -9.9786492e-05],\n",
       "        ...,\n",
       "        [ 6.6122308e-04, -1.1914626e-04, -3.9979490e-04, ...,\n",
       "          4.3226613e-04,  1.1029353e-04,  7.2092825e-04],\n",
       "        [ 9.4031449e-04, -2.4297633e-04, -3.9373591e-04, ...,\n",
       "          2.8614351e-04, -2.2604746e-04,  9.7917032e-04],\n",
       "        [ 1.2065747e-03, -3.7021402e-04, -3.7234672e-04, ...,\n",
       "          9.6005271e-05, -5.0721155e-04,  1.2582910e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 데이터셋에서 데이터 한 배치만 불러오는 방법이다.\n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "## 한 배치만 불러온 데이터를 모델에 넣어보자!\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ee771e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  18882560  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  24590049  \n",
      "=================================================================\n",
      "Total params: 80,107,489\n",
      "Trainable params: 80,107,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## 어떻게 생겼는지 보자!\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77aaad80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "488/488 [==============================] - 240s 487ms/step - loss: 3.4085 - val_loss: 3.0249\n",
      "Epoch 2/10\n",
      "488/488 [==============================] - 248s 508ms/step - loss: 2.8826 - val_loss: 2.8033\n",
      "Epoch 3/10\n",
      "488/488 [==============================] - 249s 510ms/step - loss: 2.6460 - val_loss: 2.6499\n",
      "Epoch 4/10\n",
      "488/488 [==============================] - 250s 512ms/step - loss: 2.4275 - val_loss: 2.5291\n",
      "Epoch 5/10\n",
      "488/488 [==============================] - 249s 510ms/step - loss: 2.2135 - val_loss: 2.4352\n",
      "Epoch 6/10\n",
      "488/488 [==============================] - 250s 512ms/step - loss: 2.0093 - val_loss: 2.3584\n",
      "Epoch 7/10\n",
      "488/488 [==============================] - 250s 512ms/step - loss: 1.8161 - val_loss: 2.3004\n",
      "Epoch 8/10\n",
      "488/488 [==============================] - 249s 510ms/step - loss: 1.6366 - val_loss: 2.2520\n",
      "Epoch 9/10\n",
      "488/488 [==============================] - 250s 512ms/step - loss: 1.4732 - val_loss: 2.2261\n",
      "Epoch 10/10\n",
      "488/488 [==============================] - 250s 512ms/step - loss: 1.3305 - val_loss: 2.2085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe50c0a9a00>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## optimizer와 loss등은 차차 배우니까 일단 어떤느낌인지만 알고 넘기기~\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=10, batch_size=256, validation_data = (enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02182536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    ## 테스트를 위해서 입력받은 init_sentence도 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    ## 단어 하나씩 예측해 문장을 만든다.\n",
    "    ##    1. 입력받은 문장의 텐서를 입력한다.\n",
    "    ##    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아낸다.\n",
    "    ##    3. 2에서 예측된 word index를 문장 뒤에 붙인다.\n",
    "    ##    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마친다.\n",
    "    while True:\n",
    "        ## 1\n",
    "        predict = model(test_tensor) \n",
    "        ## 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        ## 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        ## 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    ## tokenizer를 이용해 word index를 단어로 하나씩 변환한다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e18e74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you , i love you , <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 문장 생성 함수를 실행해서 i love 뒤에 어떤 문장이 나오는지 보자!!\n",
    "\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da08e11",
   "metadata": {},
   "source": [
    "# 회고!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c463d1c1",
   "metadata": {},
   "source": [
    "## <어려웠던 점>  \n",
    "=> 일단....사실 너무 지루했다....어떤 프로젝트이든 똑같겠지만! 함수랑 용어가 많고 코드가 조금 긴 편이라서 오류를 잡을 때 시간이 많이 걸렸던 것 같다. lms 노드에서도 넘어간 부분은 이해가 안되서 시간이 걸렸던 것 같다. 또한, 학습하는데 시간이 너무 많이 걸려서 다양한 시도를 해보는게 어려웠던 것 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467ea19",
   "metadata": {},
   "source": [
    "## <알아낸 점> & <아직까지 모호한 점>  \n",
    "=> 제일 처음에 sequence 라는게 나오길래 학과에서 많이 쓰이는 수열이라고 생각을 햇는데 파이썬에서 시퀀스 자료형이라는게 있는걸 보고 신기했다. 정말 사소한 거였지만 내가 알고 있는 개념과 조금 달라서 잘 기록해뒀다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c0870",
   "metadata": {},
   "source": [
    "## <시도한 것들>  \n",
    "=> 저번에 익스시간에 코드를 짜다가 중간에 그만뒀는데! 그 후에 다시 코드를 수정하려고 했는데 에러가 엄청 떠서 당황을 했다. 당황을 해서 이것저것 건들이니, 더 에러가 많이 생기고 결과가 이상해져서 아예 싹 다 지우고 다시 데이터 다듬기부터 시작했다. 그래서 시간이 많이 걸렸지만 에러가 나오지 않고 결과가 나왔다. 첨에는 val_loss값이 높아서 epochs을 10으로 설정하고 가능할까 생각을 했지만! hidden size를 1024에서 2048로 변경했더니 loss값이 학습을 할 때마다 눈에 확연하게 떨어졌다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca1a31",
   "metadata": {},
   "source": [
    "## <자기 다짐>  \n",
    "=> 마지막 i love you, 다음에 나오는 결과가 너무 귀엽게 나와서 밤에 늦게까지 한 보람이 있었던 것 같다. 제발....노드를 미루지 말고...미리하자....그래도 이번에는 미리 시작을 했지만! 노드를 미루니까 급하게 코드를 수정하기 애매하고 학습시간이 오래걸려서 애를 먹었던 것 같다! 미루지말자....제발....ㅎ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
